{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nIn this notebook, we are delving into the world of Generative Adversarial Networks (GANs), specifically focusing on generating images that resemble the distinctive style of the renowned impressionist painter Claude Monet. The task is to create a model capable of translating common photos into images mirroring Monet's unique artistic essence, using a dataset provided on Kaggle which includes a collection of Monet’s paintings and a diverse set of photos.\n\nThe dataset is divided into two main categories:\n1. Monet Paintings – present in both JPEG and TFRecord formats.\n2. Photos – available in JPEG and TFRecord formats as well.\n\nOur objective is to eventually build a GAN model that can generate thousands of Monet style images.\n\nFirst we will perform an initial Exploratory Data Analysis (EDA) to understand the characteristics, patterns, and distributions present in the datasets before moving on to the modeling phase. We will try build a strong GAN model architecture and do analysis on its performance and try to submit the best possible score for the competition.","metadata":{}},{"cell_type":"markdown","source":"## Imports and Settings","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom tqdm.notebook import tqdm\nfrom PIL import Image\nimport shutil","metadata":{"execution":{"iopub.status.busy":"2023-09-24T05:33:43.855212Z","iopub.execute_input":"2023-09-24T05:33:43.856299Z","iopub.status.idle":"2023-09-24T05:33:43.862672Z","shell.execute_reply.started":"2023-09-24T05:33:43.856254Z","shell.execute_reply":"2023-09-24T05:33:43.861410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames[0:2]:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-24T05:33:43.867790Z","iopub.execute_input":"2023-09-24T05:33:43.868638Z","iopub.status.idle":"2023-09-24T05:33:45.212464Z","shell.execute_reply.started":"2023-09-24T05:33:43.868602Z","shell.execute_reply":"2023-09-24T05:33:45.211317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1. Load the Datasets","metadata":{}},{"cell_type":"code","source":"monet_jpg_dir = '/kaggle/input/gan-getting-started/monet_jpg'\nphoto_jpg_dir = '/kaggle/input/gan-getting-started/photo_jpg'\n\nmonet_files = os.listdir(monet_jpg_dir)\nphoto_files = os.listdir(photo_jpg_dir)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T05:33:45.214432Z","iopub.execute_input":"2023-09-24T05:33:45.218577Z","iopub.status.idle":"2023-09-24T05:33:45.228400Z","shell.execute_reply.started":"2023-09-24T05:33:45.218549Z","shell.execute_reply":"2023-09-24T05:33:45.227300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Explore Image Count and Dimensions\n\nLet's explore the number of images in each category and the dimensions of a few images to understand the variety in the datasets.","metadata":{}},{"cell_type":"code","source":"print(f'Total Monet Paintings: {len(monet_files)}')\nprint(f'Total Photos: {len(photo_files)}')\n\nfor file in monet_files[:5]:\n    image = plt.imread(os.path.join(monet_jpg_dir, file))\n    print(f'Monet Image {file} Dimensions: {image.shape}')\n    \nfor file in photo_files[:5]:\n    image = plt.imread(os.path.join(photo_jpg_dir, file))\n    print(f'Photo Image {file} Dimensions: {image.shape}')","metadata":{"execution":{"iopub.status.busy":"2023-09-24T05:33:45.230270Z","iopub.execute_input":"2023-09-24T05:33:45.230715Z","iopub.status.idle":"2023-09-24T05:33:45.255616Z","shell.execute_reply.started":"2023-09-24T05:33:45.230680Z","shell.execute_reply":"2023-09-24T05:33:45.254607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Display Sample Images\n\nVisualizing a few images from both categories will provide insights into the stylistic and structural differences between Monet's paintings and the photos.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 5, figsize=(20, 8))\nfig.suptitle('Sample Images from Datasets')\n\nfor idx, file in enumerate(monet_files[:5]):\n    image_path = os.path.join(monet_jpg_dir, file)\n    image = plt.imread(image_path)\n    axes[0, idx].imshow(image)\n    axes[0, idx].set_title('Monet Painting')\n    axes[0, idx].axis('off')\n\nfor idx, file in enumerate(photo_files[:5]):\n    image_path = os.path.join(photo_jpg_dir, file)\n    image = plt.imread(image_path)\n    axes[1, idx].imshow(image)\n    axes[1, idx].set_title('Photo')\n    axes[1, idx].axis('off')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T05:33:45.258314Z","iopub.execute_input":"2023-09-24T05:33:45.258661Z","iopub.status.idle":"2023-09-24T05:33:46.778130Z","shell.execute_reply.started":"2023-09-24T05:33:45.258628Z","shell.execute_reply":"2023-09-24T05:33:46.777173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Color Distribution Analysis\n\nAnalyzing the color distributions in both Monet's paintings and the photos can help us understand the predominant color schemes and variations.","metadata":{}},{"cell_type":"code","source":"def plot_color_distribution(image_dir, file_list, title):\n    color_list = ['Reds', 'Greens', 'Blues']\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    fig.suptitle(title)\n\n    for idx, color in enumerate(color_list):\n        color_distribution = []\n\n        for file in file_list:\n            image_path = os.path.join(image_dir, file)\n            image = plt.imread(image_path)\n            color_distribution.extend(image[:, :, idx].flatten())\n\n        sns.histplot(color_distribution, bins=256, color=color[0].lower(), ax=axes[idx], kde=True)\n        axes[idx].set_title(color)\n        axes[idx].set_xlim([0, 256])\n\n    plt.show()\n\nplot_color_distribution(monet_jpg_dir, monet_files[:5], 'Color Distribution in Monet Paintings')\nplot_color_distribution(photo_jpg_dir, photo_files[:5], 'Color Distribution in Photos')","metadata":{"execution":{"iopub.status.busy":"2023-09-24T05:33:46.779217Z","iopub.execute_input":"2023-09-24T05:33:46.779583Z","iopub.status.idle":"2023-09-24T05:34:24.004189Z","shell.execute_reply.started":"2023-09-24T05:33:46.779550Z","shell.execute_reply":"2023-09-24T05:34:24.003224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This initial exploration provides insight into the structural and stylistic elements of the datasets. Understanding the variations in image dimensions, color distributions, and visual patterns will aid in designing a more robust and accurate GAN model in subsequent steps. The next steps would include pre-processing the images, developing the GAN model, and training it to generate images that harmoniously blend the realistic aspects of photos with Monet's artistic flair.","metadata":{}},{"cell_type":"markdown","source":"## Image Size Analysis","metadata":{}},{"cell_type":"code","source":"def analyze_image_sizes(image_dir, file_list):\n    dimensions_list = []\n\n    for file in file_list:\n#         file = file.numpy().decode('utf-8')\n        image_path = os.path.join(image_dir, file)\n        image = plt.imread(image_path)\n        dimensions_list.append(image.shape[:2])\n\n    return dimensions_list\n\nmonet_sizes = analyze_image_sizes(monet_jpg_dir, monet_files)\nphoto_sizes = analyze_image_sizes(photo_jpg_dir, photo_files)\n\nprint(f'Unique Dimensions in Monet Paintings: {set(monet_sizes)}')\nprint(f'Unique Dimensions in Photos: {set(photo_sizes)}')","metadata":{"execution":{"iopub.status.busy":"2023-09-24T05:34:24.005909Z","iopub.execute_input":"2023-09-24T05:34:24.006272Z","iopub.status.idle":"2023-09-24T05:34:33.707094Z","shell.execute_reply.started":"2023-09-24T05:34:24.006238Z","shell.execute_reply":"2023-09-24T05:34:33.706018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modal Building and Training\n\nLet's now build and train our GAN model to generate new images in Monet's style.\n\n## Model Architecture\n\nWe'll use a Pix2Pix GAN architecture with a generator and discriminator model. This is based on the paper ['Image-to-Image Translation with Conditional Adversarial Networks'](https://arxiv.org/abs/1611.07004).\n\nThe generator model will be a modified U-Net architecture with downsampling and upsampling layers to translate images from the photo domain to the Monet domain. \n\nThe discriminator model will be a convolutional PatchGAN classifier that distinguishes real from synthesized Monet images.","metadata":{}},{"cell_type":"code","source":"generator = keras.Sequential()\n\ngenerator.add(layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\", input_shape=[256,256,3]))\ngenerator.add(layers.LeakyReLU(alpha=0.2))\n\n# Downsampling\ngenerator.add(layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"))\ngenerator.add(layers.LeakyReLU(alpha=0.2))\ngenerator.add(layers.Conv2D(256, kernel_size=4, strides=2, padding=\"same\"))\ngenerator.add(layers.LeakyReLU(alpha=0.2))\n\n# Upsampling         \ngenerator.add(layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"))\ngenerator.add(layers.LeakyReLU(alpha=0.2))\ngenerator.add(layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\"))\ngenerator.add(layers.LeakyReLU(alpha=0.2))\n\n# Final Upsampling layer to bring the image back to 256x256\ngenerator.add(layers.Conv2DTranspose(32, kernel_size=4, strides=2, padding=\"same\"))\ngenerator.add(layers.LeakyReLU(alpha=0.2))\n\n# Output layer\ngenerator.add(layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"tanh\"))\n\nprint(generator.summary())","metadata":{"execution":{"iopub.status.busy":"2023-09-24T05:38:33.737373Z","iopub.execute_input":"2023-09-24T05:38:33.738321Z","iopub.status.idle":"2023-09-24T05:38:33.933289Z","shell.execute_reply.started":"2023-09-24T05:38:33.738288Z","shell.execute_reply":"2023-09-24T05:38:33.932451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"discriminator = keras.Sequential()\n\ndiscriminator.add(layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\", input_shape=[256,256,3]))\ndiscriminator.add(layers.LeakyReLU(alpha=0.8))\ndiscriminator.add(layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"))\ndiscriminator.add(layers.LeakyReLU(alpha=0.2))\ndiscriminator.add(layers.Conv2D(256, kernel_size=4, strides=2, padding=\"same\"))\ndiscriminator.add(layers.LeakyReLU(alpha=0.2))\n\ndiscriminator.add(layers.Conv2D(512, kernel_size=4, strides=1, padding=\"same\"))\ndiscriminator.add(layers.LeakyReLU(alpha=0.2))\n\n# Output layer\ndiscriminator.add(layers.Conv2D(1, kernel_size=4, strides=1, padding=\"same\"))\n\nprint(discriminator.summary())","metadata":{"execution":{"iopub.status.busy":"2023-09-24T05:45:34.913652Z","iopub.execute_input":"2023-09-24T05:45:34.914023Z","iopub.status.idle":"2023-09-24T05:45:35.017682Z","shell.execute_reply.started":"2023-09-24T05:45:34.913992Z","shell.execute_reply":"2023-09-24T05:45:35.016926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loss Functions\n\nWe will optimize the models using a combined loss function for the generator and discriminator.\n\nThe discriminator loss will be a binary cross-entropy loss for classifying real vs. fake images.\n\nThe generator loss will combine:\n- GAN Loss: The adversarial generator loss from trying to fool the discriminator \n- Cycle Consistency Loss: Enforce output image is mapped back to original\n- Identity Loss: Preserve color composition between input and output\n\nThis full objective function will allow the generator to produce varied Monet-style images that retain key elements from the original photos.","metadata":{}},{"cell_type":"code","source":"def discriminator_loss(real_output, generated_output):\n    bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n    real_loss = bce(tf.ones_like(real_output), real_output)\n    fake_loss = bce(tf.zeros_like(generated_output), generated_output)\n    total_loss = real_loss + fake_loss\n    return total_loss\n\ndef generator_loss(generated_output):\n    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(generated_output), generated_output)\n    return loss\n\ndef cycle_consistency_loss(real_image, generated_image, reconstructed_image):\n    loss = tf.reduce_mean(tf.abs(real_image - reconstructed_image))\n    return LAMBDA * loss\n\ndef identity_loss(real_image, generated_image):\n    loss = tf.reduce_mean(tf.abs(real_image - generated_image))\n    return LAMBDA * 0.5 * loss","metadata":{"execution":{"iopub.status.busy":"2023-09-24T05:34:33.990467Z","iopub.execute_input":"2023-09-24T05:34:33.990786Z","iopub.status.idle":"2023-09-24T05:34:34.003843Z","shell.execute_reply.started":"2023-09-24T05:34:33.990754Z","shell.execute_reply":"2023-09-24T05:34:34.002952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training \n\nWe will train the GAN for 100 epochs with a batch size of 16. To stabilize training, we use the Adam optimizer with a learning rate of 0.0002 and betas of 0.5 and 0.999 for both models.\n\nWe will also apply gradient penalty on the discriminator to enforce a soft real/fake label distribution.\n","metadata":{}},{"cell_type":"code","source":"epochs = 100\nbatch_size = 16","metadata":{"execution":{"iopub.status.busy":"2023-09-24T05:46:11.593427Z","iopub.execute_input":"2023-09-24T05:46:11.593789Z","iopub.status.idle":"2023-09-24T05:46:11.599085Z","shell.execute_reply.started":"2023-09-24T05:46:11.593760Z","shell.execute_reply":"2023-09-24T05:46:11.597726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"photo_files = tf.data.Dataset.list_files(\"/kaggle/input/gan-getting-started/photo_jpg/*.jpg\")\nmonet_files = tf.data.Dataset.list_files(\"/kaggle/input/gan-getting-started/monet_jpg/*.jpg\") \n\ndef load_image(filename):\n    image = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(image)\n    image = tf.cast(image, tf.float32)\n    image = image / 127.5 - 1 # Normalize \n    return image\n\nphoto_dataset = photo_files.map(load_image)\nmonet_dataset = monet_files.map(load_image)\n\ndataset = tf.data.Dataset.zip((photo_dataset, monet_dataset))\ndataset = dataset.shuffle(buffer_size=1000).batch(batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T05:34:34.014933Z","iopub.execute_input":"2023-09-24T05:34:34.015543Z","iopub.status.idle":"2023-09-24T05:34:34.180995Z","shell.execute_reply.started":"2023-09-24T05:34:34.015509Z","shell.execute_reply":"2023-09-24T05:34:34.179671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999) \ndiscriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999)\n\ngenerator.compile(loss='binary_crossentropy', optimizer=generator_optimizer) \ndiscriminator.compile(loss='binary_crossentropy', optimizer=discriminator_optimizer)\n\ndef compute_gradient_penalty(discriminator, real_samples, fake_samples):\n    batch_size = tf.shape(real_samples)[0]\n    alpha = tf.random.uniform(shape=[batch_size, 1, 1, 1])\n    interpolated = alpha * real_samples + (1 - alpha) * fake_samples\n    \n    with tf.GradientTape() as tape:\n        tape.watch(interpolated)\n        pred = discriminator(interpolated)\n    \n    grads = tape.gradient(pred, [interpolated])[0]\n    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n    gp = tf.reduce_mean((norm - 1.0) ** 2)\n    \n    return gp\n\n@tf.function\ndef train_step(data):\n    real_images, _ = data\n    \n    # Train discriminator\n    with tf.GradientTape() as disc_tape:\n        fake_images = generator(real_images, training=True)\n        real_output = discriminator(real_images, training=True)\n        fake_output = discriminator(fake_images, training=True)\n        disc_loss = discriminator_loss(real_output, fake_output) + compute_gradient_penalty(discriminator, real_images, fake_images)\n        \n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n    \n    # Train generator\n    with tf.GradientTape() as gen_tape:\n        fake_images = generator(real_images, training=True)\n        gen_output = discriminator(fake_images, training=True)\n        gen_loss = generator_loss(gen_output)\n        \n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    \n    return gen_loss, disc_loss","metadata":{"execution":{"iopub.status.busy":"2023-09-24T05:43:24.686407Z","iopub.execute_input":"2023-09-24T05:43:24.687317Z","iopub.status.idle":"2023-09-24T05:43:24.816854Z","shell.execute_reply.started":"2023-09-24T05:43:24.687275Z","shell.execute_reply":"2023-09-24T05:43:24.815843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator_losses = []\ndiscriminator_losses = []\n\nfor epoch in range(epochs):\n    pbar = tqdm(total=len(dataset), desc=f\"Epoch {epoch}\")\n\n    gen_losses_epoch = []\n    disc_losses_epoch = []\n    \n    for step, batch in enumerate(dataset):\n        gen_loss, disc_loss = train_step(batch)\n        gen_losses_epoch.append(gen_loss)\n        disc_losses_epoch.append(disc_loss)\n        pbar.set_description(f\"Epoch {epoch}, Step {step}, Generator Loss: {gen_loss}, Discriminator Loss: {disc_loss}\")\n        pbar.update(1)\n        \n    generator_losses.append(tf.reduce_mean(gen_losses_epoch))\n    discriminator_losses.append(tf.reduce_mean(disc_losses_epoch))\n    \n    pbar.close()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T05:43:29.951168Z","iopub.execute_input":"2023-09-24T05:43:29.951559Z","iopub.status.idle":"2023-09-24T05:45:05.384074Z","shell.execute_reply.started":"2023-09-24T05:43:29.951527Z","shell.execute_reply":"2023-09-24T05:45:05.383091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Results","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.plot(generator_losses, label='Generator Loss')\nplt.plot(discriminator_losses, label='Discriminator Loss')\nplt.title('Training Losses')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T05:45:26.083756Z","iopub.execute_input":"2023-09-24T05:45:26.084154Z","iopub.status.idle":"2023-09-24T05:45:26.377712Z","shell.execute_reply.started":"2023-09-24T05:45:26.084121Z","shell.execute_reply":"2023-09-24T05:45:26.376808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Results\n\n","metadata":{}},{"cell_type":"code","source":"output_folder_path = '/kaggle/working/generated_images'\nif not os.path.exists(output_folder_path):\n    os.makedirs(output_folder_path)\n\nfor i, photo_image in enumerate(photo_dataset):\n    generated_image = generator(np.expand_dims(photo_image, axis=0), training=False)\n    \n    if i < 5:\n        plt.figure(figsize=(10,5))\n\n        plt.subplot(1, 2, 1)\n        plt.imshow(photo_image * 0.5 + 0.5)\n        plt.title('Original Photo')\n        plt.axis('off')\n\n        plt.subplot(1, 2, 2)\n        plt.imshow(np.squeeze(generated_image) * 0.5 + 0.5)\n        plt.title('Generated Image')\n        plt.axis('off')\n\n        plt.show()\n        plt.tight_layout()\n    \n    output_image_path = os.path.join(output_folder_path, f'generated_{i}.jpg')\n    generated_image = np.squeeze(generated_image) * 127.5 + 127.5\n    Image.fromarray(generated_image.astype(np.uint8)).save(output_image_path)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T05:36:12.419689Z","iopub.execute_input":"2023-09-24T05:36:12.420048Z","iopub.status.idle":"2023-09-24T05:36:53.483043Z","shell.execute_reply.started":"2023-09-24T05:36:12.420014Z","shell.execute_reply":"2023-09-24T05:36:53.481189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission\n\nThe submission file must be named images.zip, containing a zip file of 7,000-10,000 images sized 256x256.","metadata":{}},{"cell_type":"code","source":"zip_file_path = '/kaggle/output/images.zip'\nshutil.make_archive(zip_file_path, 'zip', output_folder_path)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T05:36:53.484067Z","iopub.status.idle":"2023-09-24T05:36:53.485335Z","shell.execute_reply.started":"2023-09-24T05:36:53.485069Z","shell.execute_reply":"2023-09-24T05:36:53.485096Z"},"trusted":true},"execution_count":null,"outputs":[]}]}